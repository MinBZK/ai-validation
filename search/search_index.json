{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"AI Validation","text":"<p>Here we are documenting the processes of the AI Validation Team at the Ministry of the Interior and Kingdom Relations (Min BZK) in The Netherlands.</p> <p>Disclaimer</p> <p>This documentation is under development</p>"},{"location":"#contribute","title":"Contribute?","text":"<p>We work open source at GitHub.</p>"},{"location":"ADRs/0001-adrs/","title":"ADR-0001 ADRs","text":""},{"location":"ADRs/0001-adrs/#context","title":"Context","text":"<p>In modern software development practices, the use of Architecture Decision Records (ADRs) has become increasingly common. ADRs are documents that capture important architectural decisions made during the development process. These decisions play a crucial role in guiding the development team and ensuring consistency and coherence in the architecture of the software system.</p>"},{"location":"ADRs/0001-adrs/#assumptions","title":"Assumptions","text":"<ol> <li>ADRs provide a structured way to document and communicate architectural decisions.</li> <li>Publishing ADRs publicly fosters transparency and facilitates collaboration among team members and stakeholders.</li> <li>ADRs help in onboarding new team members by providing insights into past decisions and their rationale.</li> </ol>"},{"location":"ADRs/0001-adrs/#decision","title":"Decision","text":"<p>We will utilize ADRs in our team to document and communicate architectural decisions effectively. Furthermore, we will publish these ADRs publicly to promote transparency and facilitate collaboration.</p>"},{"location":"ADRs/0001-adrs/#template","title":"Template","text":"<p>Use the template below to add an ADR:</p> <pre><code># ADR-XXXX Title\n\n## Context\n\nWhat is the issue that we're seeing that is motivating this decision or change?\n\n## Assumptions\n\nAnything that could cause problems if untrue now or later. (optional)\n\n## Decision\n\nWhat is the change that we're proposing and/or doing?\n\n## Risks\n\nAnything that could cause malfunction, delay, or other negative impacts. (optional)\n\n## Consequences\n\nWhat becomes easier or more difficult to do because of this change?\n\n## More Information\n\nProvide additional evidence/confidence for the decision outcome\nLinks to other decisions and resources might here appear as well. (optional)\n</code></pre>"},{"location":"ADRs/0002-code-platform/","title":"ADR-0002 Code Platform","text":""},{"location":"ADRs/0002-code-platform/#context","title":"Context","text":"<p>In the landscape of software development, the choice of coding platform significantly impacts developer productivity, collaboration, and code quality. it's crucial to evaluate and select a coding platform that aligns with our development needs and fosters efficient workflows.</p>"},{"location":"ADRs/0002-code-platform/#assumptions","title":"Assumptions","text":"<p>The following assumptions are made:</p> <ul> <li>Our work should be visible to external teams to promote transparency and facilitate collaboration.</li> <li>The coding platform should be easily available for developers.</li> <li>The coding platform should offers collaboration tools between developers and the community.</li> <li>The coding platform should offers security and dependency management tools.</li> <li>The pricing model should be suitable for our budget and needs. Currently meaning no budgets.</li> </ul>"},{"location":"ADRs/0002-code-platform/#decision","title":"Decision","text":"<p>After careful consideration and evaluation of various options like GitHub, GitLab and BitBucket, we propose adopting GitHub as our primary coding platform. The decision is based on the following factors:</p> <p>Costs: There are currently no costs associate in using GitHub for our usecases.</p> <p>Features and Functionality: GitHub offers a comprehensive set of features essential for modern software development and collaboration with external teams, including version control, code review, issue tracking, continuous integration, and deployment automation.</p> <p>Security: GitHub offers a complete set of security features essential to secure development like dependency management and security scanning.</p> <p>Community and Ecosystem: GitHub boasts a vibrant community and ecosystem, facilitating knowledge sharing, collaboration, and access to third-party tools, and services that can enhance our development workflows. Within our organization we have easy access to the team managing the GitHub organization.</p> <p>Usability and User Experience: A user-friendly interface and intuitive workflows are essential for maximizing developer productivity and minimizing onboarding time. GitHub offers a streamlined user experience and customizable workflows that align with our team's preferences and practices.</p>"},{"location":"ADRs/0002-code-platform/#risks","title":"Risks","text":"<p>Currently the organization of MinBZK on GitHub does not have a lot of <code>people</code> indicating that our team is an early adapter of the platform within the organization. This might impact our features due to cost constrains.</p>"},{"location":"ADRs/0002-code-platform/#consequences","title":"Consequences","text":"<p>If we choose another tool in the future we need to migrate our codebase, and potentially need to rewrite some specific GitHub features that cannot be used in another tool.</p>"},{"location":"ADRs/0002-code-platform/#more-information","title":"More Information","text":"<p>Alternatives considered:</p> <ul> <li>BitBucket</li> <li>GitLab</li> <li>Forgejo</li> </ul>"},{"location":"ADRs/0003-ci-cd/","title":"ADR-0003 CI/CD Tooling","text":""},{"location":"ADRs/0003-ci-cd/#context","title":"Context","text":"<p>Our development team wants to implement a CI/CD solution to streamline the build, testing, and deployment workflows of our software products. Currently, our codebase resides on GitHub, and we leverage Kubernetes as our chosen orchestration platform, managed by the DigiLab platform team.</p>"},{"location":"ADRs/0003-ci-cd/#decision","title":"Decision","text":"<p>We will use the following tools for CI/CD pipeline:</p> <ul> <li>Continuous Integration (CI): GitHub Actions will be employed to facilitate the automated testing of our applications.</li> <li>Continuous Deployment (CD): We will utilize Flux for managing the deployment process of our applications. Flux reads from github to deploy.</li> </ul>"},{"location":"ADRs/0003-ci-cd/#consequences","title":"Consequences","text":"<p>GitHub Actions aligns with our existing infrastructure, ensuring seamless integration with our codebase and minimizing operational overhead. GitHub Actions' specific syntax for CI results in vendor lock-in, necessitating significant effort to migrate to an alternative CI system in the future.</p> <p>Flux, being a GitOps operator for Kubernetes, offers a declarative approach to managing deployments, enhancing reliability and repeatabilty within our Kubernetes ecosystem.</p>"},{"location":"ADRs/0004-software-hosting-platform/","title":"ADR-0004 Software hosting platform","text":""},{"location":"ADRs/0004-software-hosting-platform/#context","title":"Context","text":"<p>Our team recognizes the necessity of a platform to run our software, as our local machines lack the capacity to handle certain workloads effectively. We have evaluated several options available to us:</p> <ol> <li>Digilab Kubernetes</li> <li>Logius Kubernetes</li> <li>SSC-ICT VMs</li> <li>ODC Noord</li> </ol>"},{"location":"ADRs/0004-software-hosting-platform/#assumptions","title":"Assumptions","text":"<p>We operate under the following assumptions:</p> <ul> <li>High availability is not a critical requirement for our software.</li> <li>Our team prioritizes low maintenance solutions.</li> </ul>"},{"location":"ADRs/0004-software-hosting-platform/#decision","title":"Decision","text":"<p>We will use Digilab Kubernetes for our workloads.</p>"},{"location":"ADRs/0004-software-hosting-platform/#consequences","title":"Consequences","text":"<p>By choosing Digilab Kubernetes, we gain access to a namespace within their managed Kubernetes cluster. However, it's important to note that Digilab does not provide any guarantees regarding the availability of the cluster. Should our software require higher availability assurances, we may need to explore alternative solutions.</p>"},{"location":"ADRs/0005-python-tooling/","title":"ADR-0005 Python coding standard and tools","text":""},{"location":"ADRs/0005-python-tooling/#context","title":"Context","text":"<p>In modern software development, maintaining code quality is crucial for readability, maintainability, and collaboration. Python, being a dynamically typed language, requires robust tooling to ensure code consistency and type safety. Manual enforcement of coding standards is time-consuming and error-prone. Hence, adopting automated tooling to streamline this process is imperative.</p>"},{"location":"ADRs/0005-python-tooling/#decision","title":"Decision","text":"<p>We will use these standards and tools for our own projects:</p> <ul> <li>Google style guide</li> <li>Ruff<ul> <li>Rules: [I, SIM, B, UP, F, E]</li> <li>Formatter</li> </ul> </li> <li>Pyright: A static type checker for Python, ensuring type safety and reducing potential runtime errors.</li> <li>pre-commit: A framework for managing and maintaining multi-language pre-commit hooks.</li> </ul> <p>Working with external projects these coding standards will not always be possible. but we will try to integrate them as much as possible.</p>"},{"location":"ADRs/0005-python-tooling/#consequences","title":"Consequences","text":"<p>Improved Code Quality: Adoption of these tools will lead to improved code quality, consistency, and maintainability across the project.</p> <p>Enhanced Developer Productivity: Automated code formatting and static type checking will reduce manual effort and free developers to focus more on coding logic rather than formatting and type-related issues.</p> <p>Reduced Bug Incidence: Static typing and linting will catch potential bugs and issues early in the development process, reducing the likelihood of runtime errors and debugging efforts.</p> <p>Standardized Development Workflow: By integrating pre-commit hooks, the development workflow will be standardized, ensuring that all developers follow the same code quality standards.</p>"},{"location":"ADRs/0006-agile-tooling/","title":"ADR-0006 Agile tooling","text":""},{"location":"ADRs/0006-agile-tooling/#context","title":"Context","text":"<p>Our development team wants to enhance transparency and productivity in our software development processes. We are using GitHub for version control and collaboration. However, to further streamline our process, there is a need to incorporate tooling for managing the effort of our team.</p>"},{"location":"ADRs/0006-agile-tooling/#decision","title":"Decision","text":"<p>We will use GitHub Projects as our agile process tool</p>"},{"location":"ADRs/0006-agile-tooling/#consequences","title":"Consequences","text":"<p>GitHub Projects seamlessly integrates with our existing GitHub repositories, allowing us to manage our Agile processes. within the same ecosystem where our code resides. This integration eliminates the need for additional third-party tools, simplifying our workflow.</p>"},{"location":"ADRs/0007-commit-convention/","title":"ADR-0007 Commit convention","text":""},{"location":"ADRs/0007-commit-convention/#context","title":"Context","text":"<p>In software development, maintaining clear and consistent commit message conventions is crucial for effective collaboration, code review, and project management. Commit messages serve as a form of documentation, helping developers understand the changes introduced by each commit without having to analyze the code diff extensively.</p>"},{"location":"ADRs/0007-commit-convention/#decision","title":"Decision","text":"<p>A commit message must follow the following rules:</p> <ol> <li>The subject line (first line) MUST not be no longer than 50 characters</li> <li>The subject line MUST be in the imperative mood</li> <li>A sentences MUST have Capitalized first word</li> <li>The subject line MUST not end with a punctuation</li> <li>The body line length SHOULD be restricted to 72 characters</li> <li>The body MUST be separate by a blank line from the subject line if used</li> <li>The body SHOULD be used to explain what and why, not how.</li> <li>The body COULD end with a ticket number</li> <li>The Subject line COULD include a ticket number in the following format</li> </ol> <p>\\&lt;ref&gt;-\\&lt;ticketnumber&gt;: subject line</p> <p>An example of a commit message:</p> <p>Fix foo to enable bar</p> <p>or</p> <p>AB-1234: Fix foo to enable bar</p> <p>or</p> <p>Fix foo to enable bar</p> <p>This fixes the broken behavior of component abc caused by problem xyz.</p> <p>If we contribute to projects not started by us we try to follow the above standard unless a specific convention is obvious or required by the project.</p>"},{"location":"ADRs/0007-commit-convention/#consequences","title":"Consequences","text":"<p>In some repositories Conventional Commits are used. This ADR does not follow conventional commits.</p>"},{"location":"ADRs/0008-architectural-diagram-tooling/","title":"ADR-0008 Architectural Diagram Tooling","text":""},{"location":"ADRs/0008-architectural-diagram-tooling/#context","title":"Context","text":"<p>To communicate our designs in a graphical manner, it is of importance to draw architectural diagrams. For this we use tooling, that supports us in our work. We need to have something that is written so that it can be processed by both people and machine, and we want to have version control on our diagrams.</p>"},{"location":"ADRs/0008-architectural-diagram-tooling/#decision","title":"Decision","text":"<p>We will write our architectural diagrams in Markdown-like (.mmmd) in the Mermaid Syntax to edit these diagrams one can use the various plugins. For each project where it is needed, we will add the diagrams in the repository of the subject. The level of detail we will provide in the diagrams is according to the C4-model metamodel on architecture diagramming.</p>"},{"location":"ADRs/0008-architectural-diagram-tooling/#consequences","title":"Consequences","text":"<p>Standardized Workflow: By maintaining architecture as code, it will be standardized in our workflow.</p> <p>Version control on diagrams: By using version control, we will be able to collaborate easier on the diagrams, and we will be able to see the history of them.</p> <p>Diagrams are in .md format: By storing our diagrams next to our code, it will be where you need it the most.</p>"},{"location":"ADRs/0010-container-registry/","title":"ADR-0010 Container Registry","text":""},{"location":"ADRs/0010-container-registry/#context","title":"Context","text":"<p>Containers allow us to package and run applications in a standardized and portable way. To be able to (re)use and share images, they need to be stored in a registry that is accessible by others.</p> <p>There are many container registries. During research the following registries have been noted:</p> <p>Docker Hub, GitHub Container Registry, Amazon Elastic Container Registry (ECR), Azure Container Registry (ACR), Google Artifact Registry (GAR), Red Hat Quay, GitLab Container Registry, Harbor, Sonatype Nexus Repository Manager, JFrog Artifactory.</p>"},{"location":"ADRs/0010-container-registry/#assumptions","title":"Assumptions","text":"<ul> <li>We do not want to host our own registry.</li> <li>The images we create can be kept private or publicly shared.</li> <li>For development and testing, images should be kept private to prevent accidental use of unfinished products.</li> <li>Images we provide are safe and secure. This means a container registry should have the option to (continuously) verify the security status of an image.</li> <li>By configuration, tags can be made immutable, to prevent image tags from being overwritten.</li> <li>The registry keeps logs of events regarding containers.</li> <li>The registry needs to have a Role Based Access model.</li> <li>No additional signup is required to pull the image</li> <li>We can use a kubernetes authorisation token to pull images.</li> <li>The registry has support for scheduled deletion of images by criteria.</li> </ul>"},{"location":"ADRs/0010-container-registry/#decision","title":"Decision","text":"<p>We will use GitHub Container Registry.</p> <p>This aligns best with the previously made choices for GitHub as a code repository and CI/CD workflow.</p>"},{"location":"ADRs/0010-container-registry/#risks","title":"Risks","text":"<p>Traditionally, Docker Hub has been the place to publish images. Therefore, our images may be more difficult to discover.</p> <p>The following assumptions are not (directly) covered by the chosen registry:</p> <ul> <li>Security scans are not implemented by default, meaning we should find another solution to cover this, for example by using a GitHub Action.</li> <li>Private packages are limited by space and an additional license may be required, see Billing for GitHub Packages.</li> <li>It is unclear if it is possible to overwrite tags.</li> <li>Removing images by criteria is not implemented by default, but could be solved using a GitHub Action.</li> </ul>"},{"location":"ADRs/0010-container-registry/#consequences","title":"Consequences","text":"<p>By using GitHub Container Registry we have a container registry we can use both internally as well as share with others. This has low impact, we can always move to another registry since the Open Container Initiative is standardized.</p>"},{"location":"ADRs/0010-container-registry/#more-information","title":"More Information","text":"<p>The following sites have been consulted:</p> <ul> <li>Bluelight 'How to choose a container registry'</li> <li>G2 container-registry</li> <li>slashdot container registries</li> <li>Sourceforge Container Registries</li> <li>G2 Alternative Registries</li> <li>Security controls for container registries</li> </ul>"},{"location":"ADRs/0011-researcher-in-residence/","title":"ADR-0011 Researcher in Residence","text":""},{"location":"ADRs/0011-researcher-in-residence/#context","title":"Context","text":"<p>The AI validation team works transparently. Working with public funds warrants transparency toward the public. Additionally, being transparent aligns with the team's mission of increasing the transparency of public organizations. In line with this reasoning, it is important to be open to researchers interested in the work of the AI validation team. Allowing researchers to conduct research within the team contributes to transparency and enables external perspectives and feedback to be incorporated into the team's work.</p>"},{"location":"ADRs/0011-researcher-in-residence/#assumptions","title":"Assumptions","text":"<ul> <li>Having researchers in residence is a mechanism that facilitates transparency.</li> <li>Research enables external feedback and perspectives to be incorporated into the team's work.</li> <li>Having researchers in residence from other disciplines facilitates an interdisciplinary exchange of ideas in light of   interdisciplinary issues.</li> <li>Research outcomes enable knowledge exchange between the scientific community and public organizations, in the   Netherlands and abroad.</li> </ul>"},{"location":"ADRs/0011-researcher-in-residence/#decision","title":"Decision","text":"<p>We have decided to include a researcher in residence as a member of our team.</p> <p>The researcher in residence takes the following form:</p> <ul> <li>They join and participate in refinement meetings and other meetings of relevance.</li> <li>They share their reflections and present their (interim) research findings.</li> <li>They are provided access to the communication channel of the team (Mattermost).</li> <li>They meet with a contact person bi-weekly to discuss questions and relevant progress and findings.</li> <li>The are independent as they are employed and financed by their respective university.</li> <li>The results of their research, and thus processed data, are published in an academic journal.</li> </ul> <p>The following conditions apply to the researcher in residence.</p> <ul> <li>The research is able to access and analyze documents relevant to the research (ex. Notes/minutes taken). These   documents are submitted to a member of the team for review prior to being processed.</li> <li>Any data collected and analyzed via interviews is done on the basis of informed consent.</li> <li>The data collected is not shared with other parties and is handled ethically by the researcher.</li> <li>No information, aggregation, or summary of information from the communication channel of the team (Mattermost) is   collected, processed, or analyzed by the researcher.</li> </ul>"},{"location":"ADRs/0011-researcher-in-residence/#risks","title":"Risks","text":"<p>Risks around a potential chilling effect (team members not feeling free to express themselves) are mitigated by the conditions we impose. In light of aforementioned form and conditions above, we see no further significant risks.</p>"},{"location":"ADRs/0011-researcher-in-residence/#consequences","title":"Consequences","text":"<p>Including a researcher in residence makes it easier for them to conduct research within both the team and the wider organization where the AI validation team operates. This benefits the quality of the research findings and the feedback provided to the team and organization.</p>"},{"location":"About/contact/","title":"Contact","text":"<p>Contact us at ai-validatie@minbzk.nl.</p>"},{"location":"About/team/","title":"Our Team","text":""},{"location":"About/team/#robbert-bos","title":"Robbert Bos","text":"<p>Product Owner</p> <p></p> <p>Robbert has been on a mission for over 15 years to enhance the transparency and collaboration within AI projects. Before joining this team, he founded several data science and tech companies (partly) dedicated to this cause. Robbert is passionate about solving complex problems where he connects business needs with technology and involves others in how these solutions can improve their work.</p> <p> robbertbos</p> <p> Robbert Bos</p>"},{"location":"About/team/#lucas-haitsma","title":"Lucas Haitsma","text":"<p>Researcher in Residence</p> <p></p> <p>Lucas is PhD candidate conducting research into the regulation and governance of algorithmic discrimination by supervision and enforcement organizations. Lucas is our Researcher in Residence.</p> <p> Lucas Haitsma</p> <p> rug.nl</p>"},{"location":"About/team/#berry-den-hartog","title":"Berry den Hartog","text":"<p>Engineer</p> <p></p> <p>Berry is a software engineer passionate about problem-solving and system optimization, with expertise in Go, Python, and C++. Specialized in architecting high-volume data processing systems and implementing Lean-Agile and DevOps practices. Experienced in managing end-to-end processes from hardware provisioning to software deployment and release.</p> <p> berrydenhartog</p> <p> Berry den Hartog</p>"},{"location":"About/team/#anne-schuth","title":"Anne Schuth","text":"<p>Engineering Manager</p> <p></p> <p>Anne used to be a Machine Learning Engineering Manager at Spotify and previously held roles at DPG Media, Blendle, and Google AI. He holds a PhD from the University of Amsterdam.</p> <p> anneschuth</p> <p> Anne Schuth</p> <p> anneschuth.nl</p>"},{"location":"About/team/#christopher-spelt","title":"Christopher Spelt","text":"<p>Engineer</p> <p></p> <p>After graduating in pure mathematics, Christopher transitioned into machine learning. He is passionate about solving complex problems, especially those that have a societal impact. My expertise lies in math, machine learning theory and I'm skilled in Python.</p> <p> ChristopherSpelt</p> <p> Christopher Spelt</p>"},{"location":"About/team/#willy-tadema","title":"Willy Tadema","text":"<p>AI Ethics Lead</p> <p></p> <p>Willy specializes in AI governance, AI risk management, AI assurance and ethics-by-design. She is an advocate of AI standards and a member of several ethics committees.</p> <p> FrieseWoudloper</p> <p> Willy Tadema</p>"},{"location":"About/team/#robbert-uittenbroek","title":"Robbert Uittenbroek","text":"<p>Engineer</p> <p></p> <p>Robbert is a highly enthusiastic full-stack engineer with a Bachelor's degree in Computer Science from the Hanze University of Applied Sciences in Groningen. He is passionate about building secure, compliant, and ethical solutions, and thrives in collaborative environments. Robbert is eager to leverage his skills and knowledge to help shape and propel the future of IT within the government.</p> <p> uittenbroekrobbert</p> <p> Robbert Uittenbroek</p>"},{"location":"About/team/#laurens-weijs","title":"Laurens Weijs","text":"<p>Engineer</p> <p></p> <p>Laurens is a passionate guy with a love for innovation and doing things differently. With a background in Econometrics and Computer Science he loves to tackle the IT challenges of the Government by helping other people through extensive knowledge sharing on stage, building neural networks himself, or building a strong community.</p> <p> laurensWe</p> <p> Laurens Weijs</p>"},{"location":"Projects/TAD/","title":"Transparency of algorithmic decision making","text":"<p>This document contains a checklist with requirements for tools we could use to help with the transparency of algorithmic decision making.</p> <p>The requirements are based on:</p> <ul> <li>ISO 25010 standard: This standard defines the eight quality characteristics and provides a framework for evaluating software quality.</li> <li>Industry best practices: This includes a broad range of recommendations and guidelines for IT tool development and implementation.</li> <li>Common IT tool requirements: This information was gathered by analyzing various sources, such as documentation from popular IT tools, user reviews, and articles from reputable tech publications that discuss essential features and functionalities expected from different types of IT tools.</li> <li>Internal discussion and common sense: While above sources are already exhaustive, we also used team discussions and our own knowledge.</li> </ul>"},{"location":"Projects/TAD/#overview-of-requirements","title":"Overview of requirements","text":"<p>The requirements have been given a priority based on the MoSCoW scale to allow for tool comparison.</p>"},{"location":"Projects/TAD/#functionality","title":"Functionality","text":"Requirement Priority The tool allows users to conduct technical tests on algorithms or models, including assessments of performance, bias, and fairness. To facilitate these tests, users can input relevant datasets, M The tool allows users to choose which tests to perform. M The tool allows users to fill out questionnaires to conduct impact assessments for AI. For example IAMA or ALTAI. M The tool can generate a human readable report. M The tools works with a standardized report format, that it can read, write, and update. M The tool supports plugin functionality so additional tests can be added easily. S The tool allows to create custom reports based on components. S It is possible to add custom components for reports. S The tool provides detailed logging, including tracking of different model versions, changes in impact assessments, and technical test results for individual runs. S The tool supports saving progress. S The tool can be used on an isolated system without an internet connection. S The tool allows extension of report formats functionality. C The tool offers options to discuss and document conversations. For example, to converse about technical tests or to collaborate on impact assessments. C The tool operates with complete data privacy; it does not share any data or logging information. C The tool allows extension of report formats functionality. C The tool can be integrated in a CI/CD flow. C The tool can be offered as a (cloud) service where no local installation is required. C It is possible to define and automate workflows for repetitive tasks. C The tool offers pre-built connectors or low-code/no-code integration options to simplify the integration process. C The tool offers options to discuss and document conversations. For example, to converse about technical tests or to collaborate on impact assessments. C"},{"location":"Projects/TAD/#reliability","title":"Reliability","text":"Requirement Priority The tool operates consistently and reliably, meaning it delivers the same expected results every time you use it. M The tool recovers automatically from common failures. S The tool recovers from failures quickly, minimizing data loss, for example by automatically saving intermediate test progress results. S The tool handles errors gracefully and informs users of any issues. S The tool provides clear error messages and instructions for troubleshooting. S"},{"location":"Projects/TAD/#usability","title":"Usability","text":"Requirement Priority The tool possess a clean, intuitive, and visually appealing UI that follows industry standards. S The tool provides clear and consistent navigation, making it easy for users to find what they need. S The tool is responsive and provides instant feedback. S The user interface is multilingual and supports at least English. S The tool offers keyboard shortcuts for efficient interaction. C The user interface can easily be translated into other languages. C"},{"location":"Projects/TAD/#help-documentation","title":"Help &amp; Documentation","text":"Requirement Priority The tool provides comprehensive online help documentation with searchable functionalities. S The tool offers context-sensitive help within the application. C The online documentation includes video tutorials and training materials for ease of learning. C The project provides readily available customer support through various channels  (e.g., email, phone, online chat) to address user inquiries and troubleshoot issues. C"},{"location":"Projects/TAD/#performance-efficiency","title":"Performance Efficiency","text":"Requirement Priority The tool operates efficiently and minimize resource utilization. M The tool responds to user actions instantly. M The tool is scalable to accommodate increased user base and data volume. S"},{"location":"Projects/TAD/#maintainability","title":"Maintainability","text":"Requirement Priority The tool is easy to modify and maintain. M The tool adheres to industry coding standards and best practices to ensure code quality and maintainability. M The code is written in a common, widely adopted and supported and actively used and maintained programming language. M The project provides version control for code changes and rollback capabilities. M The project is open source. M It is possible to contribute to the source. S The system is modular, allowing for easy modification of individual components. S Diagnostic tools are available to identify and troubleshoot issues. S"},{"location":"Projects/TAD/#security","title":"Security","text":"Requirement Priority The tool must protect data and system from unauthorized access, use, disclosure, disruption, modification, or destruction. M Regular security audits and penetration testing are conducted. S The tool enforce authorization controls based on user roles and permissions, restricting access to sensitive data and functionalities. C Data encryption is used for sensitive information at rest and in transit. C The project allows for regular security audits and penetration testing to identify vulnerabilities and ensure system integrity. C The tool implements backup functionality to ensure data availability in case of incidents. C"},{"location":"Projects/TAD/#compatibility","title":"Compatibility","text":"Requirement Priority The tool is compatible with existing systems and infrastructure. M The tool supports industry-standard data formats and protocols. M The tool operates seamlessly on supported operating systems and hardware platforms. S The tool supports commonly used data formats (e.g., CSV, Excel, JSON) for easy data exchange with other systems and tools. S The tool integrates with existing security solutions. C"},{"location":"Projects/TAD/#accessibility","title":"Accessibility","text":"Requirement Priority The tool is accessible to users with disabilities, following relevant accessibility standards (e.g., WCAG). S"},{"location":"Projects/TAD/#portability","title":"Portability","text":"Requirement Priority The tool support a range of operating systems (e.g., Windows, macOS, Linux) commonly used within an organization. S The tool minimizes dependencies on specific hardware or software configurations, promoting flexibility. S The tool offers a cloud-based deployment option or be compatible with cloud environments for scalability and accessibility. S The tool adheres to relevant cloud security standards and best practices. S"},{"location":"Projects/TAD/#deployment","title":"Deployment","text":"Requirement Priority The tool has an easy and user-friendly installation and configuration process. S The tool has on-premise or cloud-based deployment options to cater to different organizational needs and infrastructure. S"},{"location":"Projects/TAD/#legal-compliance","title":"Legal &amp; Compliance","text":"Requirement Priority It is clear how the tool is funded to avoid improper influence due to conflicts of interest M The tool is compliant with relevant legal and regulatory requirements. S The tool adheres to (local) data privacy regulations like GDPR and CCPA, ensuring the protection of user data. S The tool implements appropriate security measures to comply with industry regulations and standards. S The tool is licensed for use within the organization according to the terms and conditions of the license agreement. S The tool respects intellectual property rights and avoid copyright infringement issues. S"},{"location":"Projects/TAD/reporting_standards/","title":"TAD Reporting standards","text":"<p>This document assesses standards that standardize the way algorithm assessments can be captured.</p>"},{"location":"Projects/TAD/reporting_standards/#background","title":"Background","text":"<p>There are many algorithm assessments (e.g. IAMA, HUIDERIA, etc.), technical tests on performance (e.g. Accuracy, TP, FP, F1, etc), fairness and bias of algorithms (e.g. SHAP) and reporting formats available. The goal is to have a way of standardizing the way these different assessments and tests can be captured.</p>"},{"location":"Projects/TAD/reporting_standards/#available-standards","title":"Available standards","text":""},{"location":"Projects/TAD/reporting_standards/#model-cards","title":"Model Cards","text":"<p>The most interesting existing capturing methods seem to be all based on Model Cards for Model Reporting, which are:</p> <p>\"Short documents accompanying trained machine learning models that provide benchmarked evaluation in a variety of conditions, such as across different cultural, demographic, or phenotypic groups (e.g., race, geographic location, sex, Fitzpatrick skin type) and intersectional groups (e.g., age and race, or sex and Fitzpatrick skin type) that are relevant to the intended application domains. Model cards also disclose the context in which models are intended to be used, details of the performance evaluation procedures, and other relevant information\", proposed by Google. Note that \"The proposed set of sections\" in the Model Cards paper \"are intended to provide relevant details to consider, but are not intended to be complete or exhaustive, and may be tailored depending on the model, context, and stakeholders.\"</p> <p>Many companies implement their own version of Model Cards, for example Meta System Cards and the tools mentioned in the next section.</p>"},{"location":"Projects/TAD/reporting_standards/#automatic-model-card-generation","title":"Automatic model card generation","text":"<p>There exist tools to (semi)-automatically generate models cards:</p> <ol> <li>Model Card Generator by US Sensus Bureau. Basic UI to create model cards and export to markdown, also hase a command line tool.</li> <li>Model Card Toolkit by Google. Automation only comes from integration with TensorFlowExtended and ML Metadata.</li> <li>VerifyML. Based on the Google toolkit, but is extended to include specific tests on fairness and bias. Technical tests can be added by users and model card schema (in protobuf) can be extended by users.</li> <li>Experimental Model Cards Tool by Hugging Face. This is the implementation of the Google paper by Hugging Face and provides information on the models available on their platform. The writing tools guides users through their model card and allows for up- and downloading from and to markdown.</li> </ol>"},{"location":"Projects/TAD/reporting_standards/#other-standards","title":"Other standards","text":"<p>A landscape analysis of ML documentation tools has been performed by Hugging Face and provides a good overview of the current landscape.</p> <p>Another interesting standard is the Algorithmic Transparency Recording Standard of the United Kingdom Goverment, which can be found here.</p>"},{"location":"Projects/TAD/reporting_standards/#proposal","title":"Proposal","text":"<p>We need a standard that captures algorithmic assessments and technical tests on model and datasets. The idea of model cards can serve as a guiding theoretical principle on how to implement such a standard. More specifically, we can draw inspiration from the existing model card schema's and implementations of VerifyML and Hugging Face. We note the following:</p> <ol> <li>None of these two standards capture algorithmic assessments.</li> <li>Only VerifyML has a specific format to capture some technical tests.</li> </ol> <p>Hence in any case we need to extend one of these standards. We propose to:</p> <ol> <li>Assess and compare these two standards</li> <li>Chose the most appropriate one to extend</li> <li>Extend (and possibly adjust) this standard to our own standard (in the form of a basic schema) that allows for capturing algorithmic assessments and standardizes the way technical tests can be captured.</li> </ol>"},{"location":"Projects/TAD/tools/","title":"Research of tools for transparency of algorithmic decision making","text":"<p>In our ongoing research on AI validation and transparency, we are seeking tools to support assessments. Ideal tools would combine various technical tests with checklists and questionnaires and have the ability to generate reports in both human-friendly and machine-exchangeable formats.</p> <p>This document contains a list of tools we have found and may want to investigate further.</p>"},{"location":"Projects/TAD/tools/#ai-verify","title":"AI Verify","text":"<p>AI Verify is an AI governance testing framework and software toolkit that validates the performance of AI systems against a set of  internationally recognised principles through standardised tests, and is consistent with international AI governance frameworks such as those from European Union, OECD and Singapore.</p> <p>Links: AI Verify Homepage, AI Verify documentation, AI Verify Github.</p>"},{"location":"Projects/TAD/tools/#to-investigate-further","title":"To investigate further","text":""},{"location":"Projects/TAD/tools/#verifyml","title":"VerifyML","text":"<p>What is it? VerifyML is an opinionated, open-source toolkit and workflow to help companies implement human-centric AI practices. It seems pretty much equivalent to AI Verify.</p> <p>Why interesting? The functionality of this toolkit seems to match closely with those of AI Verify. It has a \"git and code first approach\" and has automatic generation of model cards.</p> <p>Remarks The code seems to be last updated 2 years ago.</p> <p>Links: VerifyML, VerifyML GitHub</p>"},{"location":"Projects/TAD/tools/#ibm-research-360-toolkit","title":"IBM Research 360 Toolkit","text":"<p>What is it? Open source Python libraries that supports interpretability and explainability of datasets and machine learning models. Most relevant tookits are the AI Fairness 360 and AI Explainability 360.</p> <p>Why interesting? Seems to encompass extensive fairness and explainability tests. Codebase seems to be active.</p> <p>Remarks It comes as Python and R libraries.</p> <p>Links: AI Fairness 360 Github, AI Explainability 360 Github.</p>"},{"location":"Projects/TAD/tools/#hollisticai","title":"Hollisticai","text":"<p>What is it? Open source tool to assess and improve the trustworthiness of AI systems. Offers tools to measure and mitigate bias across numerous tasks. Will be extended to include tools for efficacy, robustness, privacy and explainability.</p> <p>Why interesting? Although it is not entirely clear what exactly this tool does (see Remarks) it does seem (according to their website) to provide reports on bias and fairness. The Github rep does not seem to include any report generating code, but mainly technical tests. Here is an example in which bias is measured in a classification model.</p> <p>Remarks Website seems to suggest the possibility to generate reports, but this is not directly reflected in the codebase. Possibly reports are only available with some sort of licenced product?</p> <p>Links: Hollisticai homepage, Hollisticai Github.</p>"},{"location":"Projects/TAD/tools/#interesting-to-mention","title":"Interesting to mention","text":"<ul> <li> <p>AI Assessment Tool Belgium. The tool is based on the ALTAI recommendations published by the European Commission. Although it only includes questionnaires it does give an interesting way of reporting the end results. Does not include any technical tests at this point.</p> </li> <li> <p>What-if. Provides interface for expanding understanding of a black-box classifaction or regression ML model. Can be accessed through TensorBoard or as an extension in a Jupyter or Colab notebook. Does not seem to be an active codebase.</p> </li> <li> <p>Aequitas. Open source bias auditing and Fair ML toolkit. This already seems to be contained within AI Verify, at least the 'fairness tree'.</p> </li> <li> <p>Facets. Open source toolkit for understanding and analyzing ML datasets. Note that does not include ML models.</p> </li> <li> <p>Fairness Indicators. Open source Python package which enables easy computation of commonly-identified fairness metrics for binary and multiclass classifiers. Part of TensorFlow. k</p> </li> <li> <p>Fairlearn. Open source Python package that empowers developers of AI systems to assess their system's fairness and mitigate any observed unfairness issuess.</p> </li> <li> <p>Dalex. The DALEX package xrays any model and helps to explore and explain its behaviour, helps to understand how complex models are working. The main function explain() creates a wrapper around a predictive model. Wrapped models may then be explored and compared with a collection of local and global explainers. Recent developments from the area of Interpretable Machine Learning/eXplainable Artificial Intelligence.</p> </li> <li> <p>SigmaRed. SigmaRed platform enables comprehensive third-party AI risk management (AI TPRM) and rapidly reduces the cycle time of conducting AI risks assessments while providing deep visibility, control, stakeholder based reporting, and detailed evidence repository. Does not seem to be open source.</p> </li> <li> <p>Anch.ai. The end-to-end cloud solution empowers global data-driven organizations to govern and deploy responsible, transparent, and explainable AI aligned with upcoming EU regulation AI Act. Does not seem to be open source.</p> </li> <li> <p>CredoAI. Credo AI is an AI governance platform that helps companies adopt, scale, and govern AI safely and effectively. Does not seem to be open source.</p> </li> </ul>"},{"location":"Projects/TAD/tools/#the-fate-system","title":"The FATE system","text":"<p>Paper by TNO about the FATE system. Acronym stands for \"FAir, Transparent and Explainable Decision Making.\"</p> <p>Tools mentioned include some of the above: Aequitas, AI Fairness 360, Dalex, Fairlean, Responsibly, and What-If-Tool</p> <p>Links: Paper, Article, Microsoft links.</p>"},{"location":"Way-of-Working/Code-Reviews/","title":"Code reviews","text":"<p>The purpose of a code review is to ensure the quality, readability, and that all requirements from the ticket have been met for a change before it gets merged into the main codebase. Additionally, code reviews are a communication tool, they allow team members to stay aware of changes being made.</p> <p>Code reviews involve having a team member examine the changes made by another team member and give feedback or ask questions if needed.</p>"},{"location":"Way-of-Working/Code-Reviews/#creating-a-pull-request","title":"Creating a Pull Request","text":"<p>We use GitHub pull requests (PR) for code reviews. You can make a draft PR if your work is still in progress. When you are done you can remove the draft status. A team member may start reviewing when the PR does not have a draft status.</p> <p>For team ADRs at least 3 accepting reviews are required, or all team members should accept if it can be expected that the ADR is controversial.</p> <p>A team ADR is an ADR made in the ai-validation repository.</p> <p>All other PRs only need at least 1 reviewer to get accepted, but can have more reviewers if desired (by either reviewer or author).</p>"},{"location":"Way-of-Working/Code-Reviews/#review-process","title":"Review process","text":"<p>By default the codeowner, indicated in the CODEOWNER file, will be requested to review. For us this is the GitHub team AI-validation. If the PR creator wants a specific team member to review, the PR creator should add the team member specifically in the reviewers section of the PR. A message in Mattermost will be posted for PRs. Then with the reaction of an emoji a reviewer will indicate they are looking at the PR.</p> <p>If the reviewer has suggestions or comments the PR creator can fix those or add comments to the suggestions. When the creator of the PR thinks he is done with the feedback he must re-request a review from the person that did the review. The reviewer must then look at the changes and approve or add more comments. This process continues untill the reviewer agrees that all is correct and approves the PR.</p> <p>Once the review is approved the reviewer checks if the branch is in sync with the main branch before merging. If not, the reviewer rebases the branch. Once the branch is in sync with main the reviewer merges the PR and checks if the deployment is successful. If the deployment is not successful the reviewer fixes it. If the PR needs more than one review, the last accepting reviewer merges the PR.</p>"},{"location":"Way-of-Working/Principles/","title":"Principles: How We Work","text":"<ol> <li>Our strong trust in the government and the dedication of people at all levels within the government organization is the basis of our actions.</li> <li>The interests of the citizen and society take precedence in all our activities.</li> <li>Learning and knowledge sharing are central: we encourage team members to take on tasks that are new or less familiar to them.</li> <li>Existing knowledge, policies, and proven methods are actively reused and shared.</li> <li>We strive for maximum openness and transparency in all our processes.</li> <li>We prefer the use and creation of Open Source Software.</li> <li>Our team members can choose to work anonymously.</li> <li>We treat each other with respect.</li> <li>Collaboration is essential to our success; we actively seek collaboration with both public and private partners.</li> </ol>"},{"location":"Way-of-Working/Onboarding/","title":"Onboarding","text":"<ul> <li>Start by setting up your dev machine.</li> <li>Then create your accounts.</li> <li>Read our ADRs.</li> <li>Read our principles.</li> <li>Finally, add yourself to our team page (you can stay anonymous if you want, see   our principles).</li> </ul>"},{"location":"Way-of-Working/Onboarding/Accounts/","title":"Accounts","text":""},{"location":"Way-of-Working/Onboarding/Accounts/#mattermost-chat","title":"Mattermost Chat","text":"<p>Make sure you have installed Mattermost, then follow these steps.</p> <ul> <li>Register on Pleio with your @rijksoverheid/@minbzk.nl email address.</li> <li>Login-&gt; Gitlab -&gt; Pleio.</li> <li>Ask a team member to add you to the RIG and AI Validation team.</li> <li>Make sure to add a recognizable profile picture</li> </ul>"},{"location":"Way-of-Working/Onboarding/Accounts/#webex","title":"Webex","text":"<p>Make sure you have installed Webex, then follow these steps.</p> <ul> <li>Create an account with your @rijksoverheid/@minbzk.nl email address.</li> <li>Ask a team member to add to you to the AI Validation team.</li> </ul>"},{"location":"Way-of-Working/Onboarding/Accounts/#github","title":"Github","text":"<p>Create or use your existing Github account.</p> <ul> <li>Add your @rijksoverheid/@minbzk.nl email address to your account.</li> <li>Or create a new account (anonymous if you want, see our Principles)</li> <li>Make sure to add a profile picture</li> <li>Ask a team member to add you to the MinBZK Org.</li> <li>Ask a team member to add to you to the AI Validation Team.</li> </ul>"},{"location":"Way-of-Working/Onboarding/Accounts/#collaboration-space","title":"Collaboration Space","text":"<ul> <li>Ask any team member to add you to the Team Collaboration Space.</li> </ul>"},{"location":"Way-of-Working/Onboarding/Accounts/#open-up-your-calendar","title":"Open up your calendar","text":"<ul> <li>In Outlook, right-click your calendar</li> <li>Properties</li> <li>Enable read access</li> </ul>"},{"location":"Way-of-Working/Onboarding/Accounts/#shared-email","title":"Shared email","text":"<ul> <li>Ask a colleague to add you to the shared contact address by sending an email to Secretariat   of Digital Society.</li> <li>In Outlook, go to Account Settings and \"Add Account\", leave the password empty</li> </ul>"},{"location":"Way-of-Working/Onboarding/Accounts/#bookmark","title":"Bookmark","text":"<p>Bookmark these links in your browser:</p> <ul> <li>Team Collaboration Space</li> <li>Sprint Board</li> <li>Webex Room</li> </ul>"},{"location":"Way-of-Working/Onboarding/Dev-machine/","title":"Setting up your Dev Machine","text":"<p>We are assuming your dev machine is a Mac. This guide is rather opinionated, feel free to have your own opinion, and feel free to contribute! Contributing can be done by clicking \"edit\" top right and by making a pull request on this repository.</p>"},{"location":"Way-of-Working/Onboarding/Dev-machine/#things-that-should-have-been-default-on-mac","title":"Things that should have been default on Mac","text":"<ul> <li>Keep awake with Amphetamine</li> <li>Office DisplayLink software</li> <li> <p>Homebrew as the missing Package Manager</p> <pre><code>/bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\"\n</code></pre> </li> <li> <p>Rectange</p> <pre><code>brew install --cask rectangle\n</code></pre> </li> </ul>"},{"location":"Way-of-Working/Onboarding/Dev-machine/#citrix-workspace","title":"Citrix workspace","text":"<ul> <li>Citrix workspace</li> <li>Flex2Rijk to login</li> </ul>"},{"location":"Way-of-Working/Onboarding/Dev-machine/#communication","title":"Communication","text":"<ul> <li> <p>WebEx for video conferencing</p> <pre><code>brew install --cask webex\n</code></pre> </li> <li> <p>Mattermost for team communication</p> <pre><code>brew install --cask mattermost\n</code></pre> </li> </ul>"},{"location":"Way-of-Working/Onboarding/Dev-machine/#terminal-and-shell","title":"Terminal and shell","text":"<ul> <li> <p>Iterm2</p> <pre><code>brew install --cask iterm2\n</code></pre> </li> <li> <p>Oh My Zsh</p> <pre><code>/bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/ohmyzsh/ohmyzsh/master/tools/install.sh)\"\n</code></pre> </li> <li> <p>Autosuggestions for zsh</p> <pre><code>git clone https://github.com/zsh-users/zsh-autosuggestions ~/.oh-my-zsh/custom/plugins/zsh-autosuggestions\n</code></pre> </li> <li> <p>Fish shell like syntax highlighting for Zsh</p> <pre><code>brew install zsh-syntax-highlighting\n</code></pre> </li> <li> <p>Add plugins to your shell in <code>~/.zshrc</code></p> <pre><code>plugins = (\n    # other plugins...\n    zsh-autosuggestions\n    kubectl\n    docker\n    docker-compose\n    pyenv\n    z\n)\n</code></pre> </li> <li> <p>Touch ID in Terminal</p> </li> </ul>"},{"location":"Way-of-Working/Onboarding/Dev-machine/#coding","title":"Coding","text":"<ul> <li> <p>Sourcetree</p> <pre><code>brew install --cask sourcetree\n</code></pre> </li> <li> <p>Pyenv</p> <pre><code>brew install pyenv\n</code></pre> </li> <li> <p>pyenv virtualenv</p> <pre><code>brew install pyenv-virtualenv\n</code></pre> </li> <li> <p>Xcode Command Line Tools</p> <pre><code>xcode-select --install\n</code></pre> </li> <li> <p>TabbyML Opensource, self-hosted AI coding assistant</p> <p>We can not just use hosted versions of coding assistants because of privacy and copyright issues. We can however use self-hosted coding assistants provided they are trained on data with permissive licenses.</p> <p>StarCoder (1-7B) models are all trained on version 1.2 of The Stack dataset. It boils down to all open GitHub code with permissive licenses (193 licenses in total). Minus opt-out requests.</p> <p>Code Lama and Deepseek models are not clear enough about their data licenses.</p> <pre><code>brew install tabbyml/tabby/tabby\ntabby serve --device metal --model TabbyML/StarCoder-3B\n</code></pre> <p>Then configure your IDE by installing a plugin.</p> </li> <li> <p>Sign commits using SSH</p> </li> </ul>"}]}